# -*- coding: utf-8 -*-
"""Data_Science_Bootcamp_Project - Wenqi Xu & Jen Martinez

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zbw2Pep3hHwrjlXVA6eGQ8JdmksSJOAg

# Data Science Challenge
"""

## Feel free to add other libraries you are going to use in the notebook whic hare not present here.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as mno
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score, f1_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from imblearn.over_sampling import SMOTE
import collections
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from imblearn.under_sampling import RandomUnderSampler
from sklearn.impute import KNNImputer
from sklearn.ensemble import AdaBoostClassifier
from sklearn.utils.class_weight import compute_class_weight

"""## Data Description

Column | Description
:---|:---
`id` | The unique ID assigned to every restaurant.
`name` | The name of the restaurant.
`location` | The location of the restaurant.
`phone` | The phone number of the restaurant.
`table_bookings` | Indicates if the restaurant takes online reservations.
`online_ordering` | Indicates if the restaurant takes online orders.
`restaurant_type` | The type of restaurant. Values are Casual Dining, Cafe, Quick Bites, etc.
`restaurant_operation` | The primary operation of the restaurant. For example, ‘Delivery’.
`primary_cuisine` | The name of the primary cuisine offered by the restaurant. Values are American, Mexican, etc.
`popular_dishes` | The number of popular dishes offered by the restaurant.
`cuisines_offered` | The number of cuisines offered by the restaurant.
`rating` | The rating of the restaurant.
`votes` | The number of customer votes received by the restaurant
`dining_cost` (Target Variable)| It indicates if the classification of dining costs at the restaurant. Values are 0 (Budget), 1 (Expensive).

### You will use the train.csv file to train and generate prediction on test.csv file

## Data Wrangling
"""

# Dataset is already loaded below
data = pd.read_csv("train.csv")

data.head()

data.isna().sum()

#Explore columns
data.columns

#Describe the dataset
data.describe()

### Perform some statistical analysis
data.info()

### See if null values exists for it
data.isna().sum()

### Feel free to explore and do anything you feel relevant

"""## Exploratory Data Analysis

1. Explore the shape and see what type of problem you are solving.
2. Define the problem and shape of the dataet.
3. Look for outliers if any and see its distribution across the columns, along the columns.
4. What type of null you have (Categorical or Numerical).
5. How would fix the null values.
"""

# Check for the shape of dataset
data.shape

mno.matrix(data, figsize = (20, 6))

# Is there anything you need to be concerned of write your observation.
data.dtypes

### Hint: use the **mno** library when working with visualization for the NULL values.
print(data['rating'])

## Fix the NULL VALUE ISSUE...!!!!!
data.loc[data['popular_dishes'].isna(), 'popular_dishes'] = data['popular_dishes'].median()
data.loc[data['votes'].isna(), 'votes'] = data['votes'].median()
data.loc[data['cuisines_offered'].isna(), 'cuisines_offered'] = data['cuisines_offered'].median()
data['rating'] = data['rating'].str.replace('NEW','6.0') # i think new should be changed from 6.0 to 5.0 since it's a 5.0 scale
data['rating'] = data['rating'].str.replace('-','0.0') # didn't account for the NaN values?
data = data.dropna()

from google.colab import drive
drive.mount('/content/drive')

# convert rating from string to float64
rates = data['rating'].str.slice(stop=3)
data['rating'] = rates.apply(np.float64)

data.isnull().sum()

data.dtypes

mno.matrix(data, figsize = (20, 6))

"""## Visualization

Analyse the data, try to come up with some intereszting and Insightful visualization.

Come up with different analysis and what do you infer and observe from it:
- **Univariate Analysis of all the Variables**
- **Bi-variate analysis of atleast 3 pair of variables/features**
   1.   Categorical Vs Categorical
   2.   Categorical Vs Numerical
   3.   Numerical Vs Numerical
- **Multi-variate analysis**
- Plot some Pie-Chart as well.
- Some box-plots as well.
"""

### Perform your visualization

# Check the distribution of the target variable
sns.distplot(data['dining_cost'])
plt.grid()
plt.title('Distribution of Target Variable in Data')
plt.show()
print('max:', np.max(data['dining_cost']))
print('min:', np.min(data['dining_cost']))

# Encode the categorical features in data for analysis

train_data = data.copy()

cols = ['table_bookings', 'online_ordering', 'location',
        'restaurant_type', 'restaurant_operation', 'primary_cuisine']

for c in cols: # traverse each column
    for i, item in enumerate(train_data[c].unique().tolist()):
      ## for a column create traverse all unique values in it using 'item'
        train_data.loc[train_data[c] == item, c] = i

    print("Actual values in column:", c, "\n",  data[c].unique().tolist(), '\n')
    print("Encoded values in column:", c, "\n", train_data[c].unique().tolist(), '\n')

train_data.columns

# dist plots for numeric variables
cols = ['rating', 'votes', 'popular_dishes', 'cuisines_offered']
for c in cols:
    sns.distplot(data[c])
    plt.grid()
    plt.show()

# Bi-variate analysis (bar plots for categorical and numerical features)
cols = ['table_bookings', 'online_ordering', 'location', 'rating',
        'votes', 'restaurant_type', 'restaurant_operation', 'popular_dishes', 'cuisines_offered', 'primary_cuisine']

fig, axes = plt.subplots(5, 2, figsize=(16, 16))

for i, c in enumerate(cols):
    ax = axes.ravel()[i]
    sns.barplot(x=c, y="dining_cost", ax=ax, data=train_data)

# more Bi-variate analysis (numerical)
cols = ["rating", "cuisines_offered"]
for c in cols:
    sns.jointplot(x=c, y="dining_cost", data=data, kind = 'reg', height = 5)
plt.show()

# Multi-variate analysis
plt.figure(figsize = (15,10))
sns.heatmap(train_data.corr(), cmap="CMRmap", annot=True) # this shows that we should remove ID, and possibly rating
plt.show()

sns.boxplot(x='dining_cost', y='rating', data=data)

label = data.restaurant_type.unique()
plt.pie(data.restaurant_type.value_counts(), labels = label)

label = data.location.unique()
plt.pie(data.location.value_counts(), labels = label)

label = data.online_ordering.unique()
plt.pie(data.online_ordering.value_counts(), labels = label)

label = data.restaurant_operation.unique()
plt.pie(data.restaurant_operation.value_counts(), labels = label)

sns.boxplot(x = 'dining_cost', y = 'votes', data = data)

sns.boxplot(x = 'dining_cost', y = 'popular_dishes', data = data)



"""## Feature Engineering

1. **Know the most important features of the Data**
2. **Show the Visualization for the top features**
3. **Perform certain iterations for the no. of columns you going to work and see its model performance and look for the best number of features required to build a model**

"""

data_train = data.copy()
data_train = data_train.drop(columns=['id', 'name', 'phone'])

# select categorical features
cat_cols = [c for c in data_train.columns if data_train[c].dtype == 'object'
            and c not in ['table_bookings', 'online_ordering']]
cat_data = data_train[cat_cols]
cat_cols

data_train.shape

# encode binary variables
binary_cols = ['table_bookings', 'online_ordering']
for c in binary_cols:
    data_train[c] = data_train[c].replace(to_replace=['Yes'], value=1)
    data_train[c] = data_train[c].replace(to_replace=['No'], value=0)

final_data = pd.get_dummies(data_train, columns=cat_cols, drop_first= True)

final_data.shape

final_data.head()

# select numerical features
num_cols = [c for c in data_train.columns if c not in cat_cols and c not in binary_cols and c != 'dining_cost']
num_cols

# Apply standard scaling on numeric data
scaler = StandardScaler()
scaler.fit(final_data[num_cols])
final_data[num_cols] = scaler.transform(final_data[num_cols])

"""## Modelling


Build a model that can categorizes restaurants into 'Budget' and 'Expensive' and identify how different features influence the decision. Please explain the findings effectively for technical and non-technical audiences using comments and visualizations, if appropriate.
- **Build an optimized model that effectively solves the business problem.**
- **The model will be evaluated on the basis of Accuracy.**
- **Read the test.csv file and prepare features for testing.**
"""

y = final_data['dining_cost']
X = final_data.drop(columns=['dining_cost'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
print("Training Set Dimensions:", X_train.shape)
print("Validation Set Dimensions:", X_test.shape)

from sklearn.ensemble import RandomForestRegressor

# train random forest regression model
randomf = RandomForestRegressor()

randomf.fit(X_train, y_train)

print('MAPE for train set:', np.mean(np.abs((y_train - randomf.predict(X_train))) / y_train) * 100)
print('MAPE for validation set:', np.mean(np.abs((y_test - randomf.predict(X_test))) / y_test) * 100)

# compute feature importance from random forest regression model
feature_imp=pd.DataFrame()
for feature,imp in zip(X_train.columns,randomf.feature_importances_):
    temp=pd.DataFrame([feature,imp]).T
    feature_imp=feature_imp.append(temp)
feature_imp.columns=['feature','relative_importance']
feature_imp.sort_values(by='relative_importance',inplace=True)
feature_imp.set_index('feature',inplace=True)
feature_imp.iloc[-20:,:].plot(kind='barh',figsize=(10,8))
plt.show()

# store ids and drop column
test = pd.read_csv('test.csv')
test_data = test.copy()
ids = test_data['id']
test_data = test_data.drop(columns=['id', 'name', 'phone'])

# encode binary variables
binary_cols = ['table_bookings', 'online_ordering']
for c in binary_cols:
    test_data[c] = test_data[c].replace(to_replace=['Yes'], value=1)
    test_data[c] = test_data[c].replace(to_replace=['No'], value=0)

test_data.shape

test_data.loc[test_data['popular_dishes'].isna(), 'popular_dishes'] = test_data['popular_dishes'].median()
test_data.loc[test_data['votes'].isna(), 'votes'] = test_data['votes'].median()
test_data.loc[test_data['cuisines_offered'].isna(), 'cuisines_offered'] = test_data['cuisines_offered'].median()
test_data['rating'] = test_data['rating'].str.replace('NEW','6.0')
test_data['rating'] = test_data['rating'].str.replace('-','0.0')
test_data = test_data.dropna()
rates = test_data['rating'].str.slice(stop=3)
test_data['rating'] = rates.apply(np.float64)

encoded_test_data = pd.get_dummies(test_data, columns=cat_cols, drop_first= True)

encoded_test_data.shape

test_data.dtypes

# standardize test data
encoded_test_data[num_cols] = scaler.transform(encoded_test_data[num_cols])
encoded_test_data

"""### Submission task:
- **Submit the predictions on the test dataset using the optimized model** <br/>
    For each record in the test set (`test.csv`), predict the value of the `dining_cost` variable. Submit a CSV file with a header row and one row per test entry.

The file (`submissions.csv`) should have exactly 2 columns:
   - **id**
   - **dining_cost**
"""

submission_df = pd.DataFrame(randomf.predict(encoded_test_data))

print(submission_df)

submission_df["id"] = ids

print(submission_df)

#Submission
submission_df.to_csv('submissions.csv',index=False)

"""---"""